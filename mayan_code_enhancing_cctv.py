# -*- coding: utf-8 -*-
"""Mayan_Code_Enhancing_CCTV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OVf97tRx24Dsgh45KPjB03TQyqhHi9Q-

# **Enhancing CCTV Systems with Two-Factor Authentication Using Voice and Face Recognition**

## **Author: Mayan**
**Last Updated: 17 Nov 2024|1:10 Pm**

In the implementation of a two-factor authentication (2FA) system for CCTV with face and voice recognition, the following Python libraries are crucial:

- **`opencv-python`**: Used for image and video processing.
- **`dlib`**: Provides facial landmark detection and face recognition.
- **`face-recognition`**: Simplifies the process of identifying faces.
- **`imutils`**: Offers utility functions for image operations.
- **`scipy`**: Provides scientific and numerical computations.
- **`requests`**: Facilitates HTTP requests for email notifications.
- **`SpeechRecognition`**: Converts speech to text for voice authentication.
- **`pydub`**: Allows audio file manipulation for voice analysis.

These libraries ensure efficient face and voice recognition integration, crucial for enhanced security in 2FA systems.
"""

!pip install opencv-python

!pip install dlib

!pip install face-recognition

!pip install imutils

!pip install scipy

!pip install requests

!pip install SpeechRecognition pydub

from google.colab import drive
drive.mount('/content/drive')

"""In this segment of code, the libraries dlib and face-recognition are managed using pip, Python's package manager. First, the face-recognition and dlib libraries are uninstalled using pip uninstall -y face-recognition dlib, ensuring any conflicting versions are removed. Then, dlib==19.18.0 is installed explicitly with the command pip install dlib==19.18.0 to ensure compatibility, as some versions may cause issues with the face-recognition library. Lastly, the face-recognition library is reinstalled to provide simplified face detection and recognition functionality for the two-factor authentication (2FA) system."""

!pip uninstall -y face-recognition dlib

!pip install dlib==19.18.0

!pip install face-recognition

"""This segment of code demonstrates how to send an email with an image attachment using the Mailgun API. First, the necessary Mailgun API key, domain, and recipient's email are specified. The function send_email_with_attachment(image_path, subject) is responsible for sending an email with the attached image. It first prepares the email content and then opens the specified image in binary mode (rb) to attach it to the email. Using the requests library, the script sends a POST request to the Mailgun API to trigger the email. A success message is printed if the email is sent successfully, otherwise, an error message is displayed."""

import requests

# Mailgun API setup
MAILGUN_API_KEY = '3fa059db07c52e7e18d1cebd81e7bccf-f6fe91d3-b10b41ea'  # Replace with your Mailgun API key
MAILGUN_DOMAIN = 'sandboxb43938edf367403fb9a18b0cb9dbfecd.mailgun.org'  # Replace with your Mailgun domain
OWNER_EMAIL = 'mayanbhandari@gmail.com'  # Replace with the owner's email address

# Email sending function with image attachment
def send_email_with_attachment(image_path, subject="Unknown Visitor Detected"):
    # Mailgun API URL for sending emails
    url = f"https://api.mailgun.net/v3/{MAILGUN_DOMAIN}/messages"

    # Prepare email content
    data = {
        "from": "CCTV System <noreply@yourdomain.com>",  # Change the sender email if needed
        "to": OWNER_EMAIL,
        "subject": subject,
        "text": "An unknown visitor was detected by the CCTV system."
    }

    # Open image and prepare it for attachment
    with open(image_path, "rb") as img_file:
        files = {
            "attachment": img_file
        }

        # Make the POST request to Mailgun's API to send the email
        response = requests.post(
            url,
            auth=("api", MAILGUN_API_KEY),
            data=data,
            files=files
        )

        if response.status_code == 200:
            print("Email sent successfully with attachment!")
        else:
            print(f"Error sending email: {response.status_code} - {response.text}")

"""In this code, the setup for face recognition and eye-blink detection is configured. First, necessary libraries such as cv2, face_recognition, and dlib are imported. The function load_known_faces(path, name) is used to load the images of known family members, extract their face encodings, and store them in known_face_encodings and known_face_names. The dlib library's frontal face detector and shape predictor are initialized to detect facial landmarks, crucial for further processing like eye-blink detection. Additionally, a shape predictor model file (shape_predictor_68_face_landmarks.dat) is required to identify these landmarks.


"""

# 2. Face Recognition and Eye-Blink Detection Setup
import cv2
from google.colab.patches import cv2_imshow
import face_recognition
import dlib
from scipy.spatial import distance
import time

# Load known faces
known_face_encodings = []
known_face_names = []

def load_known_faces(path, name):
    image = face_recognition.load_image_file(path)
    encoding = face_recognition.face_encodings(image)[0]
    known_face_encodings.append(encoding)
    known_face_names.append(name)

# Add known family members' faces
load_known_faces("family_member_1.jpg", "Family Member 1")
# Add other known faces as needed
#load_known_faces("family_member_2.jpg", "Family Member 2")

# Initialize facial landmarks detector and other necessary objects
detector = dlib.get_frontal_face_detector()
dlib_shape_predictor_path = "/content/shape_predictor_68_face_landmarks.dat"  # Make sure this file is available
predictor = dlib.shape_predictor(dlib_shape_predictor_path)

"""
In this code segment, video capture is initialized using OpenCV's cv2.VideoCapture() to load the video for processing. The video path is specified as /content/mayan video .mp4, which should be replaced with the correct path for your video file. The eye-blink detection parameters are set, including the eye_aspect_ratio_threshold (0.2) and eye_aspect_ratio_consec_frames (3), which define the criteria for detecting eye blinks. The function calculate_eye_aspect_ratio(eye_points) computes the eye aspect ratio (EAR) by calculating distances between specific eye landmarks. This ratio helps determine whether the eyes are closed, triggering blink detection if the threshold is exceeded.

"""

# 3. Video Capture and Eye-Blink Detection Parameters
# Video capture initialization
video_capture = cv2.VideoCapture("/content/mayan video.MP4")  # Replace with your video path

# Eye-blink detection parameters
eye_aspect_ratio_threshold = 0.2
eye_aspect_ratio_consec_frames = 3
frame_counter = 0
eye_blink_detected = False

# Function to calculate eye aspect ratio
def calculate_eye_aspect_ratio(eye_points):
    A = distance.euclidean((eye_points[1].x, eye_points[1].y), (eye_points[5].x, eye_points[5].y))
    B = distance.euclidean((eye_points[2].x, eye_points[2].y), (eye_points[4].x, eye_points[4].y))
    C = distance.euclidean((eye_points[0].x, eye_points[0].y), (eye_points[3].x, eye_points[3].y))
    return (A + B) / (2.0 * C)

"""In this code snippet, the function add_timestamp_to_frame(frame) adds a timestamp to the video frame. The timestamp is generated using the time.strftime() function, which formats the current time into a readable string in the format "YYYY-MM-DD HH:MM
". The cv2.putText() method is then used to overlay the timestamp on the frame. The position and font size are set, and the color is defined as yellow (255, 255, 0). The Y-position of the timestamp is adjusted for visibility, and the text is drawn with a thickness of 2. This function ensures that each video frame has a timestamp, providing a real-time record for the CCTV system.
"""

def add_timestamp_to_frame(frame):
    font = cv2.FONT_HERSHEY_SIMPLEX
    timestamp = time.strftime("%Y-%m-%d %H:%M:%S")  # Get the current timestamp
    text_y_position = 80  # Adjust Y-position for the timestamp
    cv2.putText(frame, timestamp, (10, text_y_position), font, 0.8, (255, 255, 0), 2, cv2.LINE_AA)
    return frame

"""This part of the code implements the main loop for face detection and eye-blink detection. It processes each video frame, detecting faces and comparing them to known family members' faces using the face_recognition library. When a face match is found, the system performs eye-blink detection by calculating the Eye Aspect Ratio (EAR) using the landmarks detected by the dlib library. If the EAR falls below a threshold, it indicates a blink. Upon detecting a blink, access is granted, and the frame is captured. If an unknown visitor is detected, an image is saved and sent to the owner via email using the Mailgun API.


"""

# 5. Main Loop for Face Detection and Blink Detection
unknown_visitor_detected = False
captured_frames = []

# Main loop to process video frames
while True:
    ret, frame = video_capture.read()
    if not ret:
        print("Error: Failed to read a frame from the video.")
        break

    rgb_frame = frame[:, :, ::-1]  # Convert frame from BGR to RGB
    face_locations = face_recognition.face_locations(rgb_frame)
    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)

    for face_encoding, face_location in zip(face_encodings, face_locations):
        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
        name = "Unknown"

        if True in matches:
            first_match_index = matches.index(True)
            name = known_face_names[first_match_index]

            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            rects = detector(gray, 0)

            for rect in rects:
                shape = predictor(gray, rect)
                left_eye = [shape.part(i) for i in range(36, 42)]
                right_eye = [shape.part(i) for i in range(42, 48)]

                left_ear = calculate_eye_aspect_ratio(left_eye)
                right_ear = calculate_eye_aspect_ratio(right_eye)
                ear = (left_ear + right_ear) / 2.0

                if ear < eye_aspect_ratio_threshold:
                    frame_counter += 1
                else:
                    if frame_counter >= eye_aspect_ratio_consec_frames:
                        eye_blink_detected = True
                    frame_counter = 0

                if eye_blink_detected:
                    print(f"Blink Detected! Access granted to {name}")

                    for point in left_eye + right_eye:
                        cv2.circle(frame, (point.x, point.y), 2, (0, 255, 0), -1)
                    cv2.putText(frame, "Blink Detected", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                    frame = add_timestamp_to_frame(frame)

                    captured_frames.append(frame)
                    if len(captured_frames) >= 3:  # Capture 3 frames after blink detection
                        break

        else:
            # If an unknown visitor is detected, capture image and send email
            if not unknown_visitor_detected:
                print("Unknown visitor detected, sending notification...")
                cv2.imwrite("/content/unknown_visitor.jpg", frame)  # Save the captured frame
                send_email_with_attachment("/content/unknown_visitor.jpg")  # Send email with attachment
                unknown_visitor_detected = True
                break

    if eye_blink_detected or len(captured_frames) >= 3 or unknown_visitor_detected:
        break  # Stop processing after either condition is met

"""This section of the code handles speech recognition for voice-based access control as part of the two-factor authentication process. It uses the speech_recognition library to convert audio into text by utilizing the Google Web Speech API. The function audio_to_text processes the audio file and returns the recognized speech as text, which is then checked against a predefined password ("open"). If the recognized text matches the password, access is granted. Otherwise, access is denied. This ensures that only authorized users can gain access after successfully completing the face recognition and eye-blink detection phases."""

import os
import numpy as np
import speech_recognition as sr
from pydub import AudioSegment

# Step 3: Define Constants
PASSWORD = "open"  # Set your password here

# Function to convert audio to text
def audio_to_text(audio_file):
    recognizer = sr.Recognizer()
    with sr.AudioFile(audio_file) as source:
        audio_data = recognizer.record(source)
    try:
        # Recognize speech using Google Web Speech API
        text = recognizer.recognize_google(audio_data)
        return text.lower()  # Convert to lowercase for comparison
    except sr.UnknownValueError:
        print("Could not understand audio")
        return None
    except sr.RequestError as e:
        print(f"Could not request results from Google Speech Recognition service; {e}")
        return None

# Function to check access
def check_access(audio_file):
    recognized_text = audio_to_text(audio_file)
    if recognized_text is not None:
        print(f"Recognized Text: {recognized_text}")
        if recognized_text == PASSWORD:
            print("Access Granted!")
        else:
            print("Access Denied!")
    else:
        print("No valid speech recognized.")

# Step 4: Upload your test audio file (WAV format)
#from google.colab import files
#uploaded = files.upload()

# Assuming the uploaded file is named 'test_audio.wav'
test_audio_file = '/content/WhatsApp Audio 2024-11-08 at 3.57.43 PM.wav'  # Change this if your file has a different name

# Step 5: Check Access with the provided audio file
check_access(test_audio_file)

"""This section of the code processes the captured frames after the successful detection of a blink or an unknown visitor. The frames are saved as images in the specified directory (/content/blink_frame_{idx}.jpg) and are displayed one by one using cv2_imshow for visual verification in the Colab environment. The video capture is then released, and all OpenCV windows are closed using cv2.destroyAllWindows to free up resources and finalize the process. These final steps ensure that the system not only captures frames when necessary but also allows for the user to view and save them."""

# 6. Final Steps: Display and Save Captured Frames
# Process and display captured frames (optional)
for idx, captured_frame in enumerate(captured_frames):
    cv2.imwrite(f"/content/blink_frame_{idx}.jpg", captured_frame)  # Save captured frames
    cv2_imshow(captured_frame)  # Display captured frames in Colab

# Release video capture and close windows
video_capture.release()
cv2.destroyAllWindows()